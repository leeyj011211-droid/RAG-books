{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e899cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87ae742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/google_books_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e116fdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>published_date</th>\n",
       "      <th>description</th>\n",
       "      <th>page_count</th>\n",
       "      <th>categories</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>preview_link</th>\n",
       "      <th>info_link</th>\n",
       "      <th>isbn_13</th>\n",
       "      <th>isbn_10</th>\n",
       "      <th>list_price</th>\n",
       "      <th>currency</th>\n",
       "      <th>buyable</th>\n",
       "      <th>search_category</th>\n",
       "      <th>thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15144</th>\n",
       "      <td>-Rdwn1PytagC</td>\n",
       "      <td>Indiana Slavic Studies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>444.0</td>\n",
       "      <td>Slavic countries</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://books.google.com/books?id=-Rdwn1PytagC&amp;...</td>\n",
       "      <td>http://books.google.com/books?id=-Rdwn1PytagC&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>language_English</td>\n",
       "      <td>http://books.google.com/books/content?id=-Rdwn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15145</th>\n",
       "      <td>_3vfMAwwezwC</td>\n",
       "      <td>The Literary Digest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Edward Jewitt Wheeler, Isaac Kaufman Funk, Wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>Literature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://books.google.com/books?id=_3vfMAwwezwC&amp;...</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>language_English</td>\n",
       "      <td>http://books.google.com/books/content?id=_3vfM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15146</th>\n",
       "      <td>yqWlwa0S5twC</td>\n",
       "      <td>Furniture Index</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>Furniture industry and trade</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://books.google.com/books?id=yqWlwa0S5twC&amp;...</td>\n",
       "      <td>https://play.google.com/store/books/details?id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>language_English</td>\n",
       "      <td>http://books.google.com/books/content?id=yqWlw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            book_id                   title subtitle  \\\n",
       "15144  -Rdwn1PytagC  Indiana Slavic Studies      NaN   \n",
       "15145  _3vfMAwwezwC     The Literary Digest      NaN   \n",
       "15146  yqWlwa0S5twC         Furniture Index      NaN   \n",
       "\n",
       "                                                 authors publisher  \\\n",
       "15144                                                NaN       NaN   \n",
       "15145  Edward Jewitt Wheeler, Isaac Kaufman Funk, Wil...       NaN   \n",
       "15146                                                NaN       NaN   \n",
       "\n",
       "      published_date description  page_count                    categories  \\\n",
       "15144           2000         NaN       444.0              Slavic countries   \n",
       "15145           1918         NaN      1068.0                    Literature   \n",
       "15146           1924         NaN      1012.0  Furniture industry and trade   \n",
       "\n",
       "       average_rating  ...  language  \\\n",
       "15144             NaN  ...        en   \n",
       "15145             NaN  ...        en   \n",
       "15146             NaN  ...        en   \n",
       "\n",
       "                                            preview_link  \\\n",
       "15144  http://books.google.com/books?id=-Rdwn1PytagC&...   \n",
       "15145  http://books.google.com/books?id=_3vfMAwwezwC&...   \n",
       "15146  http://books.google.com/books?id=yqWlwa0S5twC&...   \n",
       "\n",
       "                                               info_link isbn_13  isbn_10  \\\n",
       "15144  http://books.google.com/books?id=-Rdwn1PytagC&...     NaN      NaN   \n",
       "15145  https://play.google.com/store/books/details?id...     NaN      NaN   \n",
       "15146  https://play.google.com/store/books/details?id...     NaN      NaN   \n",
       "\n",
       "      list_price  currency buyable   search_category  \\\n",
       "15144        NaN       NaN   False  language_English   \n",
       "15145        NaN       NaN   False  language_English   \n",
       "15146        NaN       NaN   False  language_English   \n",
       "\n",
       "                                               thumbnail  \n",
       "15144  http://books.google.com/books/content?id=-Rdwn...  \n",
       "15145  http://books.google.com/books/content?id=_3vfM...  \n",
       "15146  http://books.google.com/books/content?id=yqWlw...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "323d7110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                0\n",
       "title                  8\n",
       "subtitle            9164\n",
       "authors             3525\n",
       "publisher           8057\n",
       "published_date       214\n",
       "description         6796\n",
       "page_count           214\n",
       "categories          2452\n",
       "average_rating     14290\n",
       "ratings_count          0\n",
       "language               0\n",
       "preview_link           0\n",
       "info_link              0\n",
       "isbn_13             7764\n",
       "isbn_10             8026\n",
       "list_price         12204\n",
       "currency           12204\n",
       "buyable                0\n",
       "search_category        0\n",
       "thumbnail            669\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd13ebd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://books.google.com/books/content?id=LR_VDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['thumbnail'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addd1ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14499    International bestselling author of rich dad, ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'][df['title']==\"Be Rich and Happy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020cf6d",
   "metadata": {},
   "source": [
    "International bestselling author of rich dad, poor dad! Develop new habits for financial and emotional success are you wrestling with any of these problems? You re struggling along from Paycheck to Paycheck you re earning too little to ever afford your dreams you ve got too little stored away to live comfortably in retirement then this book is for you! If you re like most of us, your years in school did little to prepare you for the challenges of the real world. They are more likely to have planted seeds of financial and emotional failure in your life. These seeds sprout later, sabotaging our most sincere attempts to get ahead and create happy, prosperous lives for ourselves and our families. This book reverses the damage. It shows you how to identify and reverse the harmful programming you unconsciously received in the classroom, and learn new habits that will set you up for financial and emotional success right now. With the blueprint in this book, you will learn attitudes and abilities that will help you not merely to survive, but to prosper, regardless of whether the economy goes up or goes down. If you want to be rich and happy, read this book! Robert kiyosaki's work in education is powerful, profound and life-changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>published_date</th>\n",
       "      <th>description</th>\n",
       "      <th>page_count</th>\n",
       "      <th>categories</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>preview_link</th>\n",
       "      <th>info_link</th>\n",
       "      <th>isbn_13</th>\n",
       "      <th>isbn_10</th>\n",
       "      <th>list_price</th>\n",
       "      <th>currency</th>\n",
       "      <th>buyable</th>\n",
       "      <th>search_category</th>\n",
       "      <th>thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>y9CExgEACAAJ</td>\n",
       "      <td>Rich Dad Poor Dad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Robert T Kiyosaki</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>In Rich Dad Poor Dad, the #1 Personal Finance ...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://books.google.com/books?id=y9CExgEACAAJ&amp;...</td>\n",
       "      <td>http://books.google.com/books?id=y9CExgEACAAJ&amp;...</td>\n",
       "      <td>9.780369e+12</td>\n",
       "      <td>0368959090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>author_Robert Kiyosaki</td>\n",
       "      <td>http://books.google.com/books/content?id=y9CEx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>C7KfPwAACAAJ</td>\n",
       "      <td>Rich Dad, Poor Dad for Teens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Robert T Kiyosaki</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://books.google.com/books?id=C7KfPwAACAAJ&amp;...</td>\n",
       "      <td>http://books.google.com/books?id=C7KfPwAACAAJ&amp;...</td>\n",
       "      <td>9.780316e+12</td>\n",
       "      <td>0316000485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>author_Robert Kiyosaki</td>\n",
       "      <td>http://books.google.com/books/content?id=C7KfP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14499</th>\n",
       "      <td>CRBj0AEACAAJ</td>\n",
       "      <td>Be Rich and Happy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Robert T Kiyosaki</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-25</td>\n",
       "      <td>International bestselling author of rich dad, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Business &amp; Economics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://books.google.com/books?id=CRBj0AEACAAJ&amp;...</td>\n",
       "      <td>http://books.google.com/books?id=CRBj0AEACAAJ&amp;...</td>\n",
       "      <td>9.789388e+12</td>\n",
       "      <td>9388423402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>author_Robert Kiyosaki</td>\n",
       "      <td>http://books.google.com/books/content?id=CRBj0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            book_id                         title subtitle            authors  \\\n",
       "14473  y9CExgEACAAJ             Rich Dad Poor Dad      NaN  Robert T Kiyosaki   \n",
       "14481  C7KfPwAACAAJ  Rich Dad, Poor Dad for Teens      NaN  Robert T Kiyosaki   \n",
       "14499  CRBj0AEACAAJ             Be Rich and Happy      NaN  Robert T Kiyosaki   \n",
       "\n",
       "      publisher published_date  \\\n",
       "14473       NaN     2019-06-19   \n",
       "14481       NaN        2004-07   \n",
       "14499       NaN     2019-04-25   \n",
       "\n",
       "                                             description  page_count  \\\n",
       "14473  In Rich Dad Poor Dad, the #1 Personal Finance ...       104.0   \n",
       "14481                                                NaN       128.0   \n",
       "14499  International bestselling author of rich dad, ...         0.0   \n",
       "\n",
       "                 categories  average_rating  ...  language  \\\n",
       "14473                   NaN             NaN  ...        en   \n",
       "14481                   NaN             NaN  ...        en   \n",
       "14499  Business & Economics             NaN  ...        en   \n",
       "\n",
       "                                            preview_link  \\\n",
       "14473  http://books.google.com/books?id=y9CExgEACAAJ&...   \n",
       "14481  http://books.google.com/books?id=C7KfPwAACAAJ&...   \n",
       "14499  http://books.google.com/books?id=CRBj0AEACAAJ&...   \n",
       "\n",
       "                                               info_link       isbn_13  \\\n",
       "14473  http://books.google.com/books?id=y9CExgEACAAJ&...  9.780369e+12   \n",
       "14481  http://books.google.com/books?id=C7KfPwAACAAJ&...  9.780316e+12   \n",
       "14499  http://books.google.com/books?id=CRBj0AEACAAJ&...  9.789388e+12   \n",
       "\n",
       "          isbn_10 list_price  currency buyable         search_category  \\\n",
       "14473  0368959090        NaN       NaN   False  author_Robert Kiyosaki   \n",
       "14481  0316000485        NaN       NaN   False  author_Robert Kiyosaki   \n",
       "14499  9388423402        NaN       NaN   False  author_Robert Kiyosaki   \n",
       "\n",
       "                                               thumbnail  \n",
       "14473  http://books.google.com/books/content?id=y9CEx...  \n",
       "14481  http://books.google.com/books/content?id=C7KfP...  \n",
       "14499  http://books.google.com/books/content?id=CRBj0...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['authors']==\"Robert T Kiyosaki\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f9fe2",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0d8ee",
   "metadata": {},
   "source": [
    "## 특수문자 제거, 통합 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b1bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 데이터 개수: 15147\n",
      "정제 후 데이터 개수: 12791\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. 원본 데이터 로드\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "def finalize_data_for_llm(df):\n",
    "    print(f\"초기 데이터 개수: {len(df)}\")\n",
    "\n",
    "    # [필터링 1] 제목(title)이 없는 행은 검색의 가치가 없으므로 삭제\n",
    "    df = df.dropna(subset=['title']).copy()\n",
    "    \n",
    "    # [필터링 2] 중복 데이터 제거 (제목과 저자가 같으면 같은 책으로 간주)\n",
    "    df = df.drop_duplicates(subset=['title', 'authors'], keep='first')\n",
    "    \n",
    "    # [필터링 3] 텍스트 정규화 및 결측치 채우기\n",
    "    cols_to_fix = ['authors', 'categories', 'description', 'subtitle', 'search_category']\n",
    "    for col in cols_to_fix:\n",
    "        df[col] = df[col].fillna('').apply(lambda x: re.sub(r'<.*?>', '', str(x)).strip())\n",
    "\n",
    "    # [필터링 4] 너무 짧은 데이터 제거\n",
    "    # 제목 + 설명이 너무 짧으면 검색했을 때 의미 있는 정보를 줄 수 없습니다.\n",
    "    df['text_len'] = df['title'].str.len() + df['description'].str.len()\n",
    "    df = df[df['text_len'] > 15].copy()\n",
    "\n",
    "    # [최종] LLM이 읽기 좋은 통합 문장 생성 (Context)\n",
    "    def make_context(row):\n",
    "        parts = [f\"Title: {row['title']}\"]\n",
    "        if row['subtitle']: parts.append(f\"Subtitle: {row['subtitle']}\")\n",
    "        if row['authors']: parts.append(f\"Author: {row['authors']}\")\n",
    "        \n",
    "        # 카테고리가 비어있으면 search_category 사용\n",
    "        category = row['categories'] if row['categories'] else row['search_category']\n",
    "        if category: parts.append(f\"Category: {category}\")\n",
    "        \n",
    "        if row['description']:\n",
    "            parts.append(f\"Description: {row['description']}\")\n",
    "        else:\n",
    "            parts.append(f\"About: This is a book related to {category}.\")\n",
    "            \n",
    "        return \" | \".join(parts)\n",
    "\n",
    "    df['context_for_embedding'] = df.apply(make_context, axis=1)\n",
    "    \n",
    "    print(f\"정제 후 데이터 개수: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# 실행\n",
    "df_final = finalize_data_for_llm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cb003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Title: Bestsellers. Author: Ivan King, bestsellers. Category: Young Adult Fiction. Description: Hear What the Critics are Saying \"Wow, what an Amazing Book for young adults; truly inspirational, very entertaining and highly thought provoking. The Path is by far one of the best Fiction books to have come out in a long time.\" -Mary Jones -Valley Daily News \"The Path is a book that will really challenge the way you view the world. A Must Read.\" -Judy B. Cohen – Elite Media Group \"Deliciously Entertaining and a very thought inducing book; I bought it for a friend as a gift and she loved it as well. It's by far one of the finest Fiction books to have come out in the last decade.\" -Dave Baker -Book Bloggers of America \"The Path is an extremely fascinating book; it really made me think. If you're looking for a book that will not only make you emotional, but will also exercise your mind, then look no further than this book. Amazing; Five Stars All The Way.\" -Debra Eisner -Literary Times Inc. \"My favorite Fiction book this year; so far we have read more than eight. Highly Recommend.\" -Emma Righter -Writers United Group \"This book reminded me why I fell in love with reading in the first place; thank you Mr. King for making such an amazing and inspiring book. Keep up with the great story telling. Ten Thumbs Up.\" -Lee Ratner –Daily Media Trends, Inc. Editorial Review The Path in itself is a journey. By the end of the book, you will feel like you have gone through an emotional and cerebral roller-coaster. This book will really make you think; but more than that, it will make you feel. The Path is a call to action for all generations, young and old. Not since Paulo Coelho's The Alchemist, has a book come out as thought provoking and inspiring. Mr. King does it again. Excellent Book! Jim S. Stein Book Description A little Boy gets lost on a path; along the way, he meets three versions of his future self and discovers the meaning of life. What lessons will he learn; what secrets are going to be revealed? Jump into the path and you will find out..... If you had the power to go back and change one decision in your life, what would it be? Favorite Quote \"Life is a comedy to those who think; a tragedy to those who feel.\" Join me on an adventure and together we will discover the true purpose of life. (bestsellers, free bestsellers, bestsellers for women, bestsellers for men) [bestsellers]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f5b985",
   "metadata": {},
   "source": [
    "# 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-cp310-win_amd64.whl (18.9 MB)\n",
      "     ---------------------------------------- 18.9/18.9 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "     -------------------------------------- 493.7/493.7 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0.0,>=0.33.4\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "     -------------------------------------- 566.1/566.1 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting langchain-core<2.0.0,>=1.2.0\n",
      "  Downloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\n",
      "     -------------------------------------- 489.1/489.1 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting tokenizers<1.0.0,>=0.19.1\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1\n",
      "  Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0\n",
      "  Using cached langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
      "Collecting PyYAML<7.0.0,>=5.3.0\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Collecting requests<3.0.0,>=2.32.5\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.13.3-cp310-cp310-win_amd64.whl (456 kB)\n",
      "     -------------------------------------- 456.7/456.7 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (2.2.6)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0\n",
      "  Using cached sqlalchemy-2.0.45-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Collecting langsmith<1.0.0,>=0.1.125\n",
      "  Downloading langsmith-0.6.1-py3-none-any.whl (282 kB)\n",
      "     -------------------------------------- 283.0/283.0 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting torch>=1.11.0\n",
      "  Using cached torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "     --------------------------------------- 12.0/12.0 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.4.1-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.22.0-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.7.0-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.1.0\n",
      "  Using cached langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0\n",
      "  Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting orjson>=3.9.14\n",
      "  Using cached orjson-3.11.5-cp310-cp310-win_amd64.whl (133 kB)\n",
      "Collecting requests-toolbelt>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting zstandard>=0.23.0\n",
      "  Using cached zstandard-0.25.0-cp310-cp310-win_amd64.whl (506 kB)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "     -------------------------------------- 131.6/131.6 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "     -------------------------------------- 152.9/152.9 kB 9.5 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.3.0-cp310-cp310-win_amd64.whl (300 kB)\n",
      "Collecting networkx>=2.5.1\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting sympy>=1.13.3\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "     ------------------------------------- 341.4/341.4 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.7/277.7 kB 8.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "     -------------------------------------- 113.6/113.6 kB 6.5 MB/s eta 0:00:00\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pydantic-core==2.41.5\n",
      "  Using cached pydantic_core-2.41.5-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Installing collected packages: mpmath, zstandard, uuid-utils, urllib3, typing-inspection, tqdm, tenacity, sympy, safetensors, regex, PyYAML, python-dotenv, pydantic-core, propcache, orjson, networkx, mypy-extensions, multidict, marshmallow, MarkupSafe, jsonpointer, idna, httpx-sse, h11, greenlet, fsspec, frozenlist, filelock, faiss-cpu, charset_normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, jsonpatch, jinja2, httpcore, anyio, aiosignal, torch, requests-toolbelt, pydantic-settings, huggingface-hub, httpx, dataclasses-json, aiohttp, tokenizers, langsmith, transformers, langchain-core, sentence-transformers, langchain-text-splitters, langchain-huggingface, langchain-classic, langchain-community\n",
      "Successfully installed MarkupSafe-3.0.3 PyYAML-6.0.3 SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.12.1 async-timeout-4.0.3 attrs-25.4.0 certifi-2026.1.4 charset_normalizer-3.4.4 dataclasses-json-0.6.7 faiss-cpu-1.13.2 filelock-3.20.2 frozenlist-1.8.0 fsspec-2025.12.0 greenlet-3.3.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 jsonpatch-1.33 jsonpointer-3.0.0 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.6 langchain-huggingface-1.2.0 langchain-text-splitters-1.1.0 langsmith-0.6.1 marshmallow-3.26.2 mpmath-1.3.0 multidict-6.7.0 mypy-extensions-1.1.0 networkx-3.4.2 orjson-3.11.5 propcache-0.4.1 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 regex-2025.11.3 requests-2.32.5 requests-toolbelt-1.0.0 safetensors-0.7.0 sentence-transformers-5.2.0 sympy-1.14.0 tenacity-9.1.2 tokenizers-0.22.2 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 typing-inspect-0.9.0 typing-inspection-0.4.2 urllib3-2.6.3 uuid-utils-0.12.0 yarl-1.22.0 zstandard-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langchain-huggingface langchain-community faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "178cd2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.2.2-py3-none-any.whl (105 kB)\n",
      "     -------------------------------------- 105.8/105.8 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain) (1.2.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Collecting langgraph<1.1.0,>=1.0.2\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "     -------------------------------------- 157.1/157.1 kB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0\n",
      "  Downloading langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 66.5/66.5 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting xxhash>=3.5.0\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-win_amd64.whl (31 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.2/46.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Collecting ormsgpack>=1.12.0\n",
      "  Downloading ormsgpack-1.12.1-cp310-cp310-win_amd64.whl (115 kB)\n",
      "     -------------------------------------- 115.9/115.9 kB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.1)\n",
      "Requirement already satisfied: idna in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2026.1.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.6.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "Successfully installed langchain-1.2.2 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 ormsgpack-1.12.1 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19526675",
   "metadata": {},
   "source": [
    "# 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb818dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human-02\\Desktop\\llm\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-02\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 시작... (약 수 분 소요)\n",
      "벡터 DB 저장 완료: faiss_book_index 폴더를 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. 임베딩 모델 설정\n",
    "# 'query: '와 'passage: ' 접두사를 사용하는 E5 모델의 특성을 반영합니다.\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # 성능 최적화를 위해 정규화 설정\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# 2. DataFrame을 LangChain 문서 형식으로 변환\n",
    "documents = []\n",
    "for _, row in df_final.iterrows():\n",
    "    # 실제 검색될 텍스트\n",
    "    page_content = row['context_for_embedding']\n",
    "    \n",
    "    # 나중에 답변에 참고할 메타데이터 (제목, 저자 등)\n",
    "    metadata = {\n",
    "        \"title\": row['title'],\n",
    "        \"authors\": row['authors'],\n",
    "        \"category\": row['search_category'],\n",
    "        \"book_id\": row['book_id']\n",
    "    }\n",
    "    documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "\n",
    "# 3. FAISS 벡터 DB 생성 (이 단계에서 임베딩 연산이 일어납니다)\n",
    "print(\"임베딩 시작... (약 수 분 소요)\")\n",
    "vector_db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 4. 로컬에 저장\n",
    "vector_db.save_local(\"faiss_book_index\")\n",
    "print(\"벡터 DB 저장 완료: faiss_book_index 폴더를 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "926a82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 다윗과 골리앗 (Malcolm Gladwell)\n",
      "내용 요약: Title: 다윗과 골리앗 | Subtitle: 강자를 이기는 약자의 기술 | Author: Malcolm Gladwell | Category: Business & Economic...\n",
      "------------------------------\n",
      "[2] Computers and Artificial Intelligence ()\n",
      "내용 요약: Title: Computers and Artificial Intelligence | Category: Artificial intelligence | About: This is a ...\n",
      "------------------------------\n",
      "[3] 그 개는 무엇을 보았나 (Malcolm Gladwell)\n",
      "내용 요약: Title: 그 개는 무엇을 보았나 | Subtitle: ch'amŭl su ŏpsi kunggŭmhan maŭm ŭi misŭt'ŏri | Author: Malcolm Gladw...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 저장된 인덱스 불러오기\n",
    "# vector_db = FAISS.load_local(\"faiss_book_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 검색 쿼리\n",
    "query = \"query : 인공지능과 미래 사회에 관한 흥미로운 소설 추천해줘\"\n",
    "\n",
    "# 유사도 검색 (상위 3개)\n",
    "docs = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"[{i+1}] {doc.metadata['title']} ({doc.metadata['authors']})\")\n",
    "    print(f\"내용 요약: {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openaiNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.8/84.8 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting tiktoken<1.0.0,>=0.7.0\n",
      "  Downloading tiktoken-0.12.0-cp310-cp310-win_amd64.whl (879 kB)\n",
      "     ------------------------------------- 879.4/879.4 kB 11.1 MB/s eta 0:00:00\n",
      "Collecting openai<3.0.0,>=1.109.1\n",
      "  Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 9.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-openai) (1.2.6)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (25.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting jiter<1,>=0.10.0\n",
      "  Using cached jiter-0.12.0-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Installing collected packages: sniffio, jiter, distro, tiktoken, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 jiter-0.12.0 langchain-openai-1.1.7 openai-2.14.0 sniffio-1.3.1 tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "978a634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (1.2.6)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-openai) (2.14.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (2.12.5)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.6.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langchain-openai langchain-core langchain-community\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Collecting ollama<1.0.0,>=0.6.0\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-ollama) (1.2.6)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.12.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (25.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.12.0)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from ollama<1.0.0,>=0.6.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (2026.1.4)\n",
      "Requirement already satisfied: idna in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (4.12.1)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.5)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.41.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.6.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\human-02\\desktop\\llm\\.venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.3.1)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d50b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 사서(Gemma2): '가족의 소중함을 느낄 수 있는 따뜻한 소설'이라고 원하시는군요!  \n",
      "\n",
      "**\"The Happiness Plan\" by Susan Mallery** 를 추천드립니다. \n",
      "\n",
      "* **세 명의 여성 친구들의 이야기:** 이 책은 Heather, Daphne, Tori라는 세 명의 여성 친구들이 각자의 어려움을 겪으면서도 서로에게 의지하며 성장하는 과정을 따뜻하게 그린 소설입니다.  \n",
      "* **가족과 우정의 중요성:**  Heather는 아버지를 찾아 나서는 과정에서 가족의 소중함을 되새기고, Daphne와 Tori 또한 사랑과 우정을 통해 힘든 시간을 이겨내며 서로에게 의지하는 따뜻한 모습이 그려집니다.\n",
      "* **희망찬 메시지:**  각 여성들이 직면하는 어려움 속에서도 긍정적인 마음으로 앞으로 나아가는 모습은 독자들에게 희망과 용기를 불어넣어줍니다.\n",
      "\n",
      "특히, 가족의 소중함을 느낄 수 있는 따뜻한 이야기라고 원하시는 점을 고려하여 \"The Happiness Plan\"를 추천했습니다.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. gemma2 모델로 설정\n",
    "llm = ChatOllama(model=\"gemma2:latest\", temperature=0.4) \n",
    "\n",
    "# 2. 리트리버 설정 (이전에 만든 vector_db 활용)\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. 텍스트 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 4. 프롬프트 구성 (gemma2의 말투를 사서로 고정)\n",
    "system_prompt = (\n",
    "    \"너는 세계 최고의 도서관 사서야. 아래 제공된 도서 정보만 참고해서 답변해.\\n\"\n",
    "    \"정보에 없는 책을 지어내지 마. 추천할 때는 제목, 저자, 이유를 꼭 포함해줘.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 5. RAG 체인 구성 (최신 invoke 방식 반영)\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 6. 대화 실행\n",
    "chat_history = []\n",
    "user_input = \"가족의 소중함을 느낄 수 있는 따뜻한 소설 추천해줘\"\n",
    "response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "print(f\"🤖 사서(Gemma2): {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c42d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 사서(Phi-3) 답변 중: 제목: Soar Like an Eagle | 내용: \"Soar Like an Eagle\" is a touching novel by Patrice Courson and Patrice Jackson that delves into the complexities of love, identity, and personal growth. The story follows Elise, who experiences firsthand what it's like to be torn between two distinct worlds due to her family background—one she has been taught as fact but must confront with newfound truths throughout life. As a young girl raised in privilege without the unconditional love of parents and growing up through various trials, Elise learns about resilience, self-discovery, and ultimately what true acceptance looks like beyond societal expectations or familial pressures.\n",
      "\n",
      "\"How to Stay Lovers While Raising Your Children\" by Anne Mayer is a guidebook for couples who want to navigate the delicate balance of maintaining their relationship while simultaneously raising children in today's fast-paced world. The book discusses how to communicate effectively, manage time and energy, handle conflicts constructively, support each other through life’s transitions, and keep love alive amidst parental duties—a must-read for couples seeking harmony between their personal bond and family responsibilities.\n",
      "\n",
      "Both of these books could offer readers a unique perspective on the trials and tribulations within relationships while also providing insight into one's own life experiences through fiction, making them thoughtful recommendations in separate realms of Family & Relationships and Self-help/Motivation respectively."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 모델을 phi3로 변경\n",
    "llm = ChatOllama(model=\"phi3\", temperature=0.5)\n",
    "\n",
    "# 2. 리트리버 설정 (이전에 생성한 vector_db 활용)\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. 문서 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"제목: {doc.metadata.get('title', 'N/A')}\\n내용: {doc.page_content}\" for doc in docs])\n",
    "\n",
    "# 4. 프롬프트 (Phi-3는 간결한 지시)\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 도서 추천 사서야. 제공된 정보만 사용해서 답변해줘.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"도서 정보: {context}\\n\\n질문: {input}\")\n",
    "])\n",
    "\n",
    "# 5. LCEL 체인 구성\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 6. 실시간 스트리밍 실행 테스트\n",
    "chat_history = []\n",
    "user_input = \"가족의 따뜻한 사랑을 느낄 수 있는 소설 추천해줘\"\n",
    "\n",
    "print(\"🤖 사서(Phi-3) 답변 중: \", end=\"\", flush=True)\n",
    "for chunk in rag_chain.stream({\"input\": user_input, \"chat_history\": chat_history}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e684569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
